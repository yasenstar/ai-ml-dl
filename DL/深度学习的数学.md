 深度学习的数学

# 1. 神经网络NN的思想

## 1.1 神经网络neural network和深度学习

### 用神经网络实现的人工智能能够自己学习过去的数据

## 1.2 神经元neuron工作的数学表示

## 1.3 激活函数：将神经元的工作一般化

### 抽象化的神经元neuron成为神经单元unit

### 点火：y = u(w1x1+w2x2+w3x3-θ), u称为单位阶跃函数 --> 激活函数，著名的是Sigmoid函数

## 1.4 什么是神经网络

### 将y=a(z)这样的神经单元连接成网络状，就形成了神经网络

### 主要考察作为基础的阶层型神经网络和尤其发展而来的卷积神经网络

#### 全连接层：前一层的神经单元与下一层的所有神经单元都有箭头连接

#### 隐藏层：负责特征提取(feature extraction)

### 深度学习是叠加了很多层的神经网络，其中著名的就是卷积神经网络

## 1.5 用恶魔来讲解神经网络的结构

## 1.6 将恶魔的工作翻译为神经网络的语言

## 1.7 网络自学习的神经网络

### 神经网络的参数确定方法分为有监督学习和无监督学习

### 有监督学习指为了确定神经网络的权重和偏置，事先给予数据，这些数据称为学习数据

### 根据给定的的学习数据确定权重和偏置，称为学习

### 神经网络的学习-模型的最优化：计算神经网络得出的预测值与正解的误差，确定使得误差总和达到最小的权重和偏置

#### 代价函数(Cost Function)：针对全部学习数据，计算预测值与正解的误差的平方（称平方误差），然后再相加，这个误差的总和
 
利用平方误差确定参数的方法在数学上称为最小二乘法


### 奇点(singularity)：表示人工智能超过人类智能的时间点

# 2. 神经网络的数学基础

## 2.1 神经网络所需的函数

### 一次函数 y=ax+b

### 二次函数

### 单位阶跃函数

### 指数函数和Sinmoid函数

### 正态分布的概率密度函数

## 2.2 有助于理解神经网络的数列和递推关系式

### 数列的通项公式

#### 将数列的第n项用一个关于n的式子表示出来，这个式子就是通项公式

### 数列与递推关系式

#### 已知首项a1以及相邻两项An, An+1的关系式，就可以确定这个数列，这个关系式就是递推关系式

### 联立递推关系式

#### 将多个数列的递推关系式联合起来组成一组

## 2.3 神经网络中经常用到的Σ符号

### Σ符号具有线性性质

## 2.4 有助于理解神经网络的向量基础

### 向量的定义：具有大小和方向的量，用箭头表示

### 向量的大小：表示向量的箭头的长度

### 向量的内积：a*b = |a|*|b|*cosθ

#### 表示两个向量在多大程度上指向相同方向

### 柯西-施瓦茨不等式

#### -|a||b|<=a*b>=|a||b|

### 内积的坐标表示

#### a=(a1,a2), b=(b1,b2), then a*b = a1b1+a2b2

#### a=(a1,a2,a3), b=(b1,b2,b3), then a*b = a1b1+a2b2+a3b3

### 向量的一般化

### 张量tensor

#### 向量概念的推广

## 2.5 有助于理解神经网络的矩阵基础

### 矩阵的乘法不满足交换律

### Hadamard乘积：相同形状的矩阵A,B，将相同位置的元素相乘产生的矩阵

### 转置矩阵Transposed Matrix：将矩阵A的第i行第j列的元素与第j行第i列的元素交换产生的矩阵

## 2.6 神经网络的导数基础

### 已知函数f(x)，求导函数f'(x)，称为对函数f(x)求导，当导数的值存在时，称函数可导

### 在平面直角坐标系中，f'(x)表示曲线的某点的切线的斜率

### 导数的线性性（和的导数为导数的和，常数倍的导数是导数的常数倍），是后面的误差反向传播法背后的主角
 (see:4.3 神经网络和误差反向传播法)
### 分数函数的导数和Sigmoid函数的导数

### 当f(x)在x=a处取最小值时，该函数在该点的切线的斜率（即导函数的值）为0

#### f'(a)=0是函数f(x)在x=a处取得最小值的必要条件

## 2.7 神经网络的偏导数基础（多变量函数）

### 对多变量函数求导，由于有多个变量，必须指明对哪一个变量进行求导，关于某个特定变量的导数称为偏导数(partial derivative)

### 多变量函数的最小值条件

#### 拉格朗日乘数法

## 2.8 误差反向传播法必需的链式法则

### 神经网络和复合函数

#### 复合函数：嵌套结构的函数f(g(x))称为f(u)和g(x)的复合函数

### 单变量函数的链式法则

#### 复合函数的导数可以像分数一样使用约分

#### 注意：这个约分的法则不适用于dx，dy的平方等情形

### 多变量函数的链式法则

#### 变量z为u,v的函数，u,v分别为x,y的函数，z关于x求导时，先对u,v求导，然后与z的相应导数相乘，最后将乘积加起来

## 2.9 梯度下降法的基础：多变量函数的近似公式

### 梯度下降法时确定神经网络的一种代表性的方法，使用时要用到多变量函数的近似公式

### 单变量函数的近似公式

### 多变量函数的近似公式

### 近似公式的向量表示

#### 泰勒展开式：近似公式的一般化公式

## 2.10 梯度下降法的含义和公式

### 应用数学最重要的任务之一就是寻找函数取最小值的点

#### 著名的找最小值的点的方法--梯度下降法

### 梯度下降法的思路

#### 在函数取最小值的点的附件，函数的增量为0（必要条件）

## 2.11 用Excel体验梯度下降法

## 2.12 最优化问题和回归分析

# 3. 神经网络的最优化

## 3.1 神经网络的参数和变量

## 3.2 神经网络的变量的关系式

## 3.3. 学习数据和正解

## 3.4 神经网络的代价函数

## 3.5 用Excel体验神经网络

# 4. 神经网络和误差反向传播法

## 4.1 梯度下降法的回顾

## 4.2 神经单元误差

## 4.3 神经网络和误差反向传播法

## 4.4 用Excel体验神经网络的误差反向传播法

# 5. 深度学习和卷积神经网络
