 Hands-On LLM
图解大模型

# 一、理解语言模型

探索大、小语言模型的内部运作机制

- GPT - Generative Pre-Trained Transformer


## 01 大语言模型简介

概述该领域和常用技术


### 01.01 什么是语言人工智能

#### John McCarthy (2007): (人工智能)是制造机器，特别是智能计算机程序的科学和工程。它与使用计算机理解人类智能的任务相似，但人工智能不必局限于生物学上可观察到的那些方法

#### 语言人工智能是人工智能的一个子领域，专注于开发能够理解、处理和生成人类语言的技术。随着机器学习方法在解决语言处理问题方面取得持续成功，语言人工智能与自然语言处理 (natural language processing, NLP) 可以互换使用。

#### 使用语言人工智能这个术语来涵盖那些在技术上可能不是LLM，但仍对该领域有重大影响的技术，比如检索系统

### 01.02 语言人工智能的近期发展史

#### Brief History

##### ~2000：词袋（非Tranformer模型）

##### 2013：word2vec（非Tranformer模型）

##### 2017：注意力（非Tranformer模型）

##### 2018：BERT（仅编码器模型）;GPT（仅解码器模型）

##### 2019

###### GPT-2（仅解码器模型）

###### DistilBERT（仅编码器模型）

###### RoBERTa（仅编码器模型）

###### T5（编码器-解码器模型）

##### 2020

###### GPT-3（仅解码器模型）

###### Switch（编码器-解码器模型）

##### 2022

###### FLAN-T5（编码器-解码器模型）

###### ChatGPT（仅解码器模型）

#### 01.02.01 将语言表示为词袋模型

##### 词袋 - bag of words

##### 第一步：分词（tokenization）

##### 分词之后，将所有不同的词组合起来，创建一个可用于表示句子的词表（vocabulary）

##### 词袋模型旨在以数字形式创建文本的表示（representation），也称为向量或向量表示，这类模型成为表示模型（representation model）

#### 01.02.02 用稠密向量嵌入获得更好的表示

##### word2vec（词向量）与2013年发布，是首批成功利用嵌入（embedding）概念捕捉文本含有的技术之一

##### 嵌入是数据的向量表示，试图捕捉数据的含义

#### 01.02.03 嵌入的类型

##### 词袋在文档层面创建嵌入

##### word2vec为每个词生成一个嵌入

#### 01.02.04 使用注意力机制编解码上下文

##### 使用RNN (recurrent neural network 循环神经网络) 可以实现文本编码的一个步骤

##### 编码：表示输入句子

##### 解码：生成输出句子

## 02 词元和嵌入

讨论这些模型的两个核心组件：词元(Token)和嵌入(Embedding)


## 03 LLM的内部机制

深入讨论模型的架构


# 二、使用预训练语言模型

通过常见用例探索如何使用LLM，使用预训练模型并展示功能，无须进行微调。


## 04 文本分类

使用语言模型进行监督分类


## 05 文本聚类和主题建模

使用语言模型进行文本聚类和主题建模


## 06 提示工程

利用嵌入模型进行文本生成


## 07 高级文本生成技术与工具

利用嵌入模型进行文本生成


## 08 语义搜索与RAG

利用嵌入模型进行语义搜索


## 09 多模态LLM

将文本生成能力扩展到视觉领域


# 三、训练和微调语言模型

通过训练和微调各种语言模型来探索高级概念


## 10 构建文本嵌入模型

探讨如何构建和微调嵌入模型


## 11 为分类任务微调表示模型

回顾如何针对分类任务微调BERT


## 12 微调生成模型

介绍几种生成模型的微调方法

