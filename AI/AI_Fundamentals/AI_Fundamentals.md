 AI Fundamentals

# 0. Credly Account Info

# 1. Intro to AI

Learning Points:

- Define artificial intelligence
- Describe three levels of artificial intelligence
- Describe the history of AI from the past to the possible future
- Define and describe machine learning
- Differentiate between structured and unstructured data
- Describe how machine learning structures data
- Describe how machine learning structures unstructured data
- Describe how machine learning uses probabilistic calculation to solve problems
- Describe three methods by which machine learning analyzes data
- Describe an ideal relationship between humans and machine learning


## 1.0 Course Overview

### About this Course

After completing this course, you should be able to:



- Define artificial intelligence
- Differentiate between AI and augmented intelligence
- Describe three levels of artificial intelligence
- Describe the history of AI, from the past to the possible future
- Define and describe machine learning
- Differentiate between structured, unstructured, and semi-structured data
- Describe how machine learning structures unstructured data
- Describe how machine learning uses probabilistic calculation to solve problems
- Describe three methods by which machine learning analyzes data
- Describe an ideal relationship between humans and machine learning


### Earn a Credential!

### Course Design

## 1.1 Module 1: What is AI?

### About this Module

#### Three levels of predictions that AI can make

##### Narrow

##### Broad

##### General

### What is AI?
 
Artificial Intelligence (AI) refers to the ability of a machine tolearn patterns andmake predictions.


#### AI does not replace human decisions; instead, AI add values to human judgment.
 
AI is a field that combinescomputer science  and robustdataset to enable problem-solving.


### What is the difference between AI and Augmented Intelligence?

#### Augmented intelligence has a modest goal of helping humans with tasks that are not practical to do.

##### Augmented intelligence allows humans to make final decisions after analyzing data, reports, and other types of data

#### Artificial intelligence has a lofty goal of mimicking human thinking and processes.

##### AI performs tasks without human intervention and completes mundane and repetitive tasks for humans

#### AI today is not mature enough to perform independent tasks such as diagnosing cancer.

#### Machines vs. Humans

##### Machines

###### Ingesting Large Amount of Data

###### Repetitive Tasks

###### Accurate

##### Humans

###### Generalizing Information

###### Creativity

###### Emotional Intelligence

### What does AI do?

#### Artificial intelligence machines (researchers call them "AI Services") don't think. They calculate.

##### They represent some of the newest, most sophisticated calculating machines in human history.
 
Some can perform what's calledmachine learning  as they acquire new data.

 
Others, using calculations arranged in ways inspired by neurons in the human brain, can even performdeep learning  with multiple levels of calculations.


#### Two parts of AI Services Calculate

##### Analysis

###### AI services can take in (or “ingest”) enormous amounts of data. They can apply mathematical calculations to analyze data, sorting and organizing it in ways that would have been considered impossible only a few years ago.

##### Prediction

###### AI services can use their data analysis to make predictions. They can, in effect, say, “Based on this information, a certain thing will probably happen.”
 
AI Services based on dataanalysis, makeprediction.


### What Predictions can AI Make?

#### Prediction aren't always accurate. But if they're correct often enough, they're useful and can save your time.

#### More ways that AI uses data to make predictions

##### Human Language

###### Online chatbots use natural language processing (NLP) to analyze poorly typed or spoken questions, then predict which answers to give on topics ranging from shipping or business hours to merchandise and sizes.

##### Vision Recognition

###### AI helps doctors identify serious diseases based on unusual symptoms and early-warning signs, and it reads speed limit and stop signs as it guides cars through traffic.

##### Fraud Detection

###### AI analyzes patterns created when thousands of bank customers make credit card purchases, then predicts which charges might be the result of identity theft.

### How is AI Evolving?

#### Narrow AI (2010-2015)

##### Narrow AI si focused on addressing a single task such as predicting your next purchase or planning your day

##### Narrow AI is scaling very quickly in the consumer world, in which there are a lot of common tasks and data to train AI systems. e.g., you can buy a book with a voice-based device.

##### Narrow AI also enables robust applications, such as using Siri on an iPhone, the Amazon recommendation engine, autonomous vehicles, and more. Narrow AI systems like Siri have conversational capabilities, but only if you stick to the script.

#### Broad AI (AI for Enterprise) (Today), We are here

##### Broad AI is a midpoint between Narrow and Genaral AI

##### Rather than being limited to a single task, Broad AI systems are more versatile and can handle a wider range of related tasks

##### Broad AI is focused on integrating AI within a specific business process where companies need business- and enterprise-specific knowledge and data to train this type of system

##### New Broad AI systems predict global weather, trace pandemics, and help businesses predict future trends

#### General AI (2050 and beyond)

##### General AI refers to machines that can perform any intellectual task that a human can

##### Currently, AI does not have the ability to think abstractly, strategize, and use previous experiences to come up with new, creative ideas as humans do, such as inventing a new product or responding to people with appropriate emotions. And don't worry, AI is nowhere near this point.

#### Artificial Superintelligence (ASI) (near the end of this century)

##### Machine might become self-aware!

## 1.2 Module 2: What are the three eras of computing?

### About this Module

#### Describe the history of AI, from the past to the possible future

### The Era of Tabulation: People have analyzed data for centuries

#### Dark Data

##### Dark Data: it's information without structure, just a huge, unsorted mess of facts.

#### Unstructured Data

##### Over 2000 years ago, tax collectors for Emperor Qin Shihuang used the abacus—a device with beads on wires—to break down tax receipts and arrange them into categories. From this, they could determine how much the Emperor should spend on building extensions to the Great Wall of China.

##### In England during the mid-1800s, Charles Babbage and Ada Lovelace designed (but never finished) what they called a “difference engine” designed to handle complex calculations using logarithms and trigonometry. Had they built it, the difference engine might have helped the English Navy build tables of ocean tides and depth soundings that could guide English sailors through rough waters.

##### By the late 1880s, people were thinking about how to develop faster systems to record data. Herman Hollerith, inspired by train conductors using holes punched in different positions on a railway ticket to record traveler details, invented the recording of data on a machine-readable punched card. Hollerith’s cards were used for the 1890 US Census, which finished months ahead of schedule and under budget. Later versions of tabulating machines had broad applications in business, such as financial accounting and data processing.

#### The word to remember across those twenty centuries is tabulate. Think of tabulation as “slicing and dicing” data to give it a structure, so that people can uncover patterns of useful information. You tabulate when you want to get a feel for what all those columns and rows of data in a table really mean.

#### Researchers call these centuries the Era of Tabulation, a time when machines helped humans sort data into structures to reveal its secrets.

#### Machines helped humans sort data into structures to reveal its secrets, that is, to reveal more insight than just simply counting to get a sum total

### The Era of Programming: Data analysis changed in the 1940s

#### During the turmoil of World War II, a new approach to dark data emerged: the Era of Programming. Scientists began building electronic computers, like the Electronic Numerical Integrator and Computer (ENIAC) at the University of Pennsylvania, that could run more than one kind of instruction (today we call those “programs”) in order to do more than one kind of calculation. ENIAC, for example, not only calculated artillery firing tables for the US Army, it worked in secret to study the feasibility of thermonuclear weapons.

##### This was a huge breakthrough. Programmable computers guided astronauts from Earth to the moon and were reprogrammed during Apollo 13’s troubled mission to bring its astronauts safely back to Earth.

#### You’ve grown up during the Era of Programming. It even drives the phone you hold in your hand. But the dark data problem has also grown. Modern businesses and technology generate so much data that even the finest programmable supercomputer can't analyze it before the “heat-death” of the universe. Electronic computing is facing a crisis.

### The Era of AI: a brief history of AI

#### The history of artificial intelligence dates back to philosophers thinking about the question, "What more can be done with the world we live in?" This question lead to discussions and the very beginning of many ideas about the possibilities involving technology.

#### 1940s: Turing Machine

##### Alan Turing publishes Computing Machinery and Intelligence. In the paper, Turing—famous for helping to break the Nazis’ Enigma code during World War II—proposes to answer the question "can machines think?" and introduces the Turing Test to determine if a computer can demonstrate the same intelligence (or the results of the same intelligence) as a human.

#### 1940s: Analogue Robots

#### 1950: Turing Test

#### 1051: Minsky Neural Net

#### 1956: Dartmouth Conference, Birth of AI

##### John McCarthy coins the term "artificial intelligence at the first-ever AI conference at Dartmouth College. Later that year, Allen Newell, J.C. Shaw, and Herbert Simon create the Logic Theorist, the first-ever running AI software program. McCarthy would go on to invent the Lisp language.

##### The researchers proposed that “every aspect of learning or any other feature of intelligence can be so precisely described that a machine can be made to simulate it.” They called their vision “artificial intelligence” and they raised millions of dollars to achieve it within 20 years. During the next two decades, they accomplished tremendous things, creating machines that could prove geometry theorems, speak simple English, and even solve word problems with algebra.

#### 1957: Checkers

#### 1960s: Semantic Networks

#### 1966: ELIZA

#### 1969: SHRDLU Born

#### 1970-80: 1st AI Winter - K9, Star Wars
 
1st AI Winter caused byhigh expectations from end users andreduced funding.


###### Limited calculating power
Today, it is important for a computer to have enough processing power and memory. Every ad you see for companies like Apple or Dell emphasizes how fast their processors run and how much data they can work with. But in 1976, scientists realized that even the most successful computers of the day, working with natural language, could only manipulate a vocabulary of about 20 words. But a task like matching the performance of the human retina might require millions of instructions per second, at a time when the world’s fastest computer could run only about a hundred. By the early 1970s, it became clear that the problem was larger than researchers imagined. There were fundamental limits that no amount of money and effort could solve.

###### Limited information storage
Even simple, commonsense reasoning requires a lot of information to back it up. But no one in 1970 knew how to build a database large enough to hold even the information known by a 2-year-old child.

#### 1982: Expert Systems - ZX81

#### 1982: Hopfield Net/Back Propagation

#### 1982 to 1993: 2nd AI Winter - Joined IBM

##### 2nd AI Winter caused by unmet expectations and computing power.

##### Over 300 AI companies shut down or went bankrupt during The Second Winter of AI.

#### 1997: Deep Blue Beats Kasparov (chess)

#### 2005: DARPA Grand Challenge (self driving vehicle)

#### 2011: Watson Wins Jeopardy (quiz show)

##### IBM Watson beats champions Ken Jennings and Brad Rutter at the US game show called Jeopardy!

#### 2016: AlphaGo (Go)

##### DeepMind's AlphaGo program, powered by a deep neural network, beats Lee Sodol, the world champion Go player, in a five-game match. The victory is significant given the huge number of possible moves as the game progresses (over 14.5 trillion after just four moves!). Google bought DeepMind for a reported USD 400 million in 2014.

#### 2017: Alpha Zero - K9 Mk 1

#### 2019: Project Debater

##### IBM unveils Project Debater, the first AI system capable of engaging with humans on complex topics in a live debate.

#### 2022: K9 Mk2

## 1.3 Module 3: Structured, Semi-Structured, and Unstructured Data

### About this Module

#### Data is raw information. Data might be facts, statistics, opinions, or any kind of content that is recorded in some format. This could include voices, photos, names, and even dance moves!

### A look at the types of Data

#### Structured Data

##### Structured data is typically categorized as quantitative data and is highly organized. Structured data is information that can be organized in rows and columns. Perhaps you've seen structured data in a spreadsheet, like Google Sheets or Microsoft Excel. Examples of structured data includes names, dates, addresses, credit card numbers, stock information.

#### Unstructured Data (Dark Data)

##### Unstructured data, also known as dark data, is typically categorized as qualitative data. It cannot be processed and analyzed by conventional data tools and methods. Unstructured data lacks any built-in organization, or structure. Examples of unstructured data include images, texts, customer comments, medical records, and even song lyrics.

#### Semi-Structured Data

##### Semi-structured data is the “bridge” between structured and unstructured data. It doesn't have a predefined data model. It combines features of both structured data and unstructured data. It's more complex than structured data, yet easier to store than unstructured data. Semi-structured data uses metadata to identify specific data characteristics and scale data into records and preset fields. Metadata ultimately enables semi-structured data to be better cataloged, searched, and analyzed than unstructured data. An example of semi-structured data is a video on a social media site. The video by itself is unstructured data, but a video typically has text for the internet to easily categorize that information, such as through a hashtag to identify a location.

### Analyzing Unstructured Data

## 1.4 Module 4: Is Machine Learning the Answer to the Unstructured Data Problem?

### About this Module

### How does ML Approach a Problem?

### ML uses Probabilistic Calculation

## 1.5 Module 5: Three Common Methods of Machine Learning

### About this Module

### Supervised Learning

### Unsupervised Learning

### Reinforcement Learning

## 1.6 Module 6: How will Machine Learning Transform Human Life?

### About this Module

### Take another look at the Three Levels of AI

## 1.7 Summary and Final Assessment

# 2. Natural Language Processing and Computer Vision

# 3. Machine Learning and Deep Learning

# 4. Run AI Models with IBM Watson Studio

# 5. AI Ethics

# 6. Your Future in AI: The Job Landscape
